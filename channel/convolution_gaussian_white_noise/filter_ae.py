# -*- coding: utf-8 -*-
"""filter_UNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13vDRMkR7UC3Nd984FN2Xhex-K1kloeAN
"""

# CHEN Junwei, EAP LAP, UC3M, junwei.chen@uc3m.es
# 240510 v4 reduce model to prevent GPU RAM exploding
# 240513 v5 refine
# 240513 v6 transplant to google colab
# 240514 v7 not subtract by mean
# 240514 AEv10
# import pandas as pd
import tensorflow as tf
from tensorflow import keras
import numpy as np
import time
import random
import h5py
import os
from tensorflow.keras import layers, losses
from tensorflow.keras.models import Model
from scipy.io import savemat

start_time0 = time.time()
# filename, _ = os.path.splitext(os.path.basename(__file__))
filename = 'AutoEncoder'


# set GPU number
os.environ['CUDA_VISIBLE_DEVICES']='0'
prefix = ''
'''
# google colab utilities
from google.colab import drive
drive.mount('/content/gdrive')
prefix = 'gdrive/MyDrive/COLAB/gauss_noise/'
'''
matfile = 'V_NOISE.mat'

# parameters
Shape = (12, 88, 88)
MyOptimizer   = keras.optimizers.Adam(learning_rate=1e-4)
MyEpochs = 1200
MyBatchSize = 24
# SConcat = 1 # 1/0 with/without skip connections
AFrame = range(120) #field_out

with h5py.File(prefix+matfile, 'r') as file:
    U0 = file['U_array'][()]
    V0 = file['V_array'][()]
    W0 = file['W_array'][()]
    U1 = file['U_ext']  [()]
    V1 = file['V_ext']  [()]
    W1 = file['W_ext']  [()]
    Um = file['Um']     [()]
    Vm = file['Vm']     [()]
    Wm = file['Wm']     [()]

U, V, W = U1, V1, W1
del U1, V1, W1
total_sam = U.shape[0]*3

def ConvUnit(x, filter_num, kernel_size):
	x = keras.layers.Conv3D(filter_num, kernel_size, activation='LeakyReLU', padding='same')(x)
#	x = keras.layers.Conv3D(filter_num, kernel_size, activation='relu', padding='same')(x)
	return x

'''
def AEDeNoise():
    input_layer = keras.Input(shape = Shape+(1,))
    x0 = input_layer
    x1 = ConvUnit(x0, 16, 3)
    x2 = keras.layers.MaxPool3D(2, padding='same')(x1)
    x2 = ConvUnit(x2, 32, 3)
    x3 = keras.layers.MaxPool3D(2, padding='same')(x2)
    x3 = ConvUnit(x3, 64, 3)
    z2 = keras.layers.Conv3DTranspose( 32, 3, strides=2, activation='relu', padding='same')(x3)
    z2 = tf.concat([x2, z2], axis=-1)
    z2 = ConvUnit(z2, 32, 3)
    z1 = keras.layers.Conv3DTranspose( 16, 3, strides=2, activation='relu', padding='same')(z2)
    z1 = tf.concat([x1, z1], axis=-1)
    z1 = ConvUnit(z1, 16, 3)
    z0 = keras.layers.Conv3D(1, 3, activation='relu', padding='same')(z1)

    model = tf.keras.Model(input_layer,z0)
    return model
'''
'''
def AEDeNoise():
    input_layer = keras.Input(shape = Shape+(1,))
    x0 = input_layer
    x1 = ConvUnit(x0, 16, 3)
    x2 = keras.layers.MaxPool3D(2, padding='same')(x1)
    x2 = ConvUnit(x2, 32, 3)
    z1 = keras.layers.Conv3DTranspose( 16, 3, strides=2, activation='relu', padding='same')(x2)
    z1 = tf.concat([x1, z1], axis=-1)
    z1 = ConvUnit(z1, 16, 3)
    z0 = keras.layers.Conv3D(1, 3, activation='relu', padding='same')(z1)

    model = tf.keras.Model(input_layer,z0)
    return model
'''

def AEDeNoise():
    input_layer = keras.Input(shape = Shape+(1,))
    x0 = input_layer
    x1 = ConvUnit(x0, 16, 3)
    x2 = keras.layers.MaxPool3D(2, padding='same')(x1)
    x2 = ConvUnit(x2, 32, 3)
    x3 = keras.layers.MaxPool3D(2, padding='same')(x2)
    x3 = ConvUnit(x3, 64, 3)
    z2 = keras.layers.Conv3DTranspose( 32, 3, strides=2, activation='LeakyReLU', padding='same')(x3)
    z2 = ConvUnit(z2, 32, 3)
    z1 = keras.layers.Conv3DTranspose( 16, 3, strides=2, activation='LeakyReLU', padding='same')(z2)
    z1 = ConvUnit(z1, 16, 3)
    z0 = keras.layers.Conv3D(1, 3, activation='LeakyReLU', padding='same')(z1)

    model = tf.keras.Model(input_layer,z0)
    return model

ae_denoise = AEDeNoise()

# ae_denoise.summary()

@tf.function
def train_stage(x):
    with tf.GradientTape() as tape:
        field    = tf.reshape(x, (MyBatchSize,)+Shape+(1,))
        field_ae = ae_denoise(field, training=True)
        loss = tf.reduce_sum(tf.square(field_ae-field))
    gradients = tape.gradient(loss, ae_denoise.trainable_variables)
    MyOptimizer.apply_gradients(zip(gradients,ae_denoise.trainable_variables))
    return loss/MyBatchSize

time_elapsed = time.time() - start_time0
print('preparation time of '+filename+': %.1fs' %time_elapsed)

# Commented out IPython magic to ensure Python compatibility.
# training
for epoch in range(MyEpochs):
    start_time = time.time()
    # sample_list=np.array(random.sample(list(range(total_sam)),total_sam))
    sample_list = range(total_sam)
    # u = np.concatenate((U-Um, V-Vm, W-Wm), axis=0)
    u = np.concatenate((U, V, W), axis=0)
    u0 = tf.cast(u[sample_list,:], tf.float32)
    for batch in range(int(total_sam/MyBatchSize)):
        flag = batch*MyBatchSize
        u    = u0[flag:flag+MyBatchSize,:]
        loss = train_stage(u)
    print('epoch: %d/%d - time: %.1fms/epoch - loss: %f'
              %(epoch+1,MyEpochs,(time.time()-start_time)*1000, loss))

# save result
'''
u_filt = ae_denoise(tf.reshape(U0[AFrame,:]-Um, (len(AFrame),)+Shape+(1,)))
u_out = np.reshape(u_filt, (len(AFrame),np.size(U0, axis=1)))+Um
v_filt = ae_denoise(tf.reshape(V0[AFrame,:]-Vm, (len(AFrame),)+Shape+(1,)))
v_out = np.reshape(v_filt, (len(AFrame),np.size(U0, axis=1)))+Vm
w_filt = ae_denoise(tf.reshape(W0[AFrame,:]-Wm, (len(AFrame),)+Shape+(1,)))
w_out = np.reshape(w_filt, (len(AFrame),np.size(U0, axis=1)))+Wm
'''
u_filt = ae_denoise(tf.reshape(U0[AFrame,:], (len(AFrame),)+Shape+(1,)))
u_out = np.reshape(u_filt, (len(AFrame),np.size(U0, axis=1)))
v_filt = ae_denoise(tf.reshape(V0[AFrame,:], (len(AFrame),)+Shape+(1,)))
v_out = np.reshape(v_filt, (len(AFrame),np.size(U0, axis=1)))
w_filt = ae_denoise(tf.reshape(W0[AFrame,:], (len(AFrame),)+Shape+(1,)))
w_out = np.reshape(w_filt, (len(AFrame),np.size(U0, axis=1)))

data_dict = {
    'U': np.transpose(u_out),
    'V': np.transpose(v_out),
    'W': np.transpose(w_out),
    }
savemat(prefix+'V_AE.mat', data_dict)

ae_denoise.save_weights(prefix+'AE_weights.h5')

time_elapsed = time.time() - start_time0
print('total time of '+filename+': %.1fs'%time_elapsed)

# tf.reset_default_graph()
# session = tf.Session()
# tf.config.experimental.set_memory_growth(gpus[0], True)
# tf.config.experimental.set_memory_growth( True)
# gpus = tf.config.experimental.list_physical_devices('GPU')
# tf.config.experimental.set_memory_growth(gpus[0], True)
